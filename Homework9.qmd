---
title: "ST558 Homework9"
format: html
editor: visual
---

# Jamin Goo (NCSU id : jgoo)

```{r}
#| message: FALSE
#| warning: FALSE

# Load necessary libraries
library(tidyverse)
library(tidymodels)
library(lubridate)
library(janitor)
library(baguette)
library(glmnet)
library(ranger)
library(rpart.plot)
```

## 1. Reading Data (Homework8)

```{r}
bike_data_raw <- read_csv("SeoulBikeData.csv", 
                          locale = locale(encoding = "latin1"))
```

## 2. EDA (Homework8)

### Checking the Data

```{r}
# 1. Check for missingness
print(colSums(is.na(bike_data_raw)))
```

All columns are 0 : There is no missing.

```{r}
# 2. Check column types and values
glimpse(bike_data_raw)
summary(bike_data_raw |> select(where(is.numeric)))
print(map(bike_data_raw |> select(where(is.character), -Date), unique))
```

```{r}
# 3, 4, 5. Convert Date, Convert to Factors, Rename
bike_data_cleaned <- bike_data_raw |>
  mutate(
    Date = dmy(Date), # 3. Convert the Date column into an actual date 
    Seasons = factor(Seasons), # 4: Turn the character variables 
    Holiday = factor(Holiday), # 4: Turn the character variables 
  ) |>
  clean_names() |> # 5: Rename all the variables to have easy to use names 
  mutate(
    functioning_day = factor(functioning_day) # 4: Turn the character variables 
  )

glimpse(bike_data_cleaned)
```

2.  

```{r}
# 6: Create summary statistics (especially related to the bike rental count)
print(
  bike_data_cleaned |>
    group_by(functioning_day) |>
    summarize(
      count = n(),
      min_bikes = min(rented_bike_count),
      mean_bikes = mean(rented_bike_count),
      max_bikes = max(rented_bike_count)
    )
)

# 6: Subset the data appropriately
bike_data_functional <- bike_data_cleaned |>
  filter(functioning_day == "Yes")

summary(bike_data_functional)

bike_data_functional_no <- bike_data_cleaned |>
  filter(functioning_day == "No")

summary(bike_data_functional_no)
```

If the bike is not functioning, the rented bike count is always 0.

So we will focus on the data where the bike is functioning for further analysis.

```{r}
# 7: To simplify our analysis, weâ€™ll summarize across the hours
bike_data_daily <- bike_data_functional |>
  group_by(date, seasons, holiday) |>
  summarize(
    # Find the sum of rented_bike_count, rainfall, and snowfall
    rented_bike_count_sum = sum(rented_bike_count),
    rainfall_sum_mm = sum(rainfall_mm),
    snowfall_sum_cm = sum(snowfall_cm),
    
    # Find the mean of all the weather related variables
    temperature_mean_c = mean(temperature_c),
    humidity_mean_percent = mean(humidity_percent),
    wind_speed_mean_m_s = mean(wind_speed_m_s),
    visibility_mean_10m = mean(visibility_10m),
    dew_point_temperature_mean_c = mean(dew_point_temperature_c),
    solar_radiation_mean_mj_m2 = mean(solar_radiation_mj_m2),
    
    .groups = 'drop' 
  )

glimpse(bike_data_daily)
```

```{r}
# 8: Recreate my basic summary stats
summary(bike_data_daily)

# Plot: Rented Bike Count vs. Mean Temperature
plot_temp <- bike_data_daily |>
  ggplot(aes(x = temperature_mean_c , y = rented_bike_count_sum)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(
    title = "Daily Bike Rentals vs. Mean Temperature",
    x = "Mean Temperature (C)",
    y = "Sum of Daily Bike Rentals"
  )
print(plot_temp)

#  correlation between my numeric variables
cor_matrix <- bike_data_daily |>
  select(where(is.numeric)) |>
  cor()
print(round(cor_matrix, 2))
```

## 3. Split the Data (Homework8)

```{r}
set.seed(10)
bike_split <- initial_split(bike_data_daily, prop = 0.75, strata = seasons)
bike_train <- training(bike_split)
bike_test <- testing(bike_split)
bike_CV_folds <- vfold_cv(bike_train, v = 10, strata = seasons)
```

## 4. MLR recipe (Homework8)

```{r}
MLR_recipe1 <- recipe(rented_bike_count_sum ~ ., data = bike_train) |>
  # Ignore the date variable for modeling
  step_date(date, features = c("dow")) |>
  step_mutate(day_type = factor(if_else(date_dow %in% c("Sat", "Sun"), 
                                        "weekend", "weekday"))) |>
  step_rm(date, date_dow) |> 
  # Standardize the numeric variables since their scales are pretty different.
  step_normalize(all_numeric_predictors()) |>
  # Create dummy variables for the seasons, holiday, and our new day type variable
  step_dummy(all_nominal_predictors())
```

## 5. Model Specifications

```{r}
# A. LASSO Model
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")

# B. Regression Tree Model
tree_spec <- decision_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) |>
  set_engine("rpart") |>
  set_mode("regression")

# C. Bagged Tree Model
bag_spec <- bag_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) |>
  set_engine("rpart", times = 50) |>
  set_mode("regression")

# D. Random Forest Model
rf_spec <- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) |>
  set_engine("ranger", importance = "impurity") |>
  set_mode("regression")
```

## 6. Workflows

```{r}
lasso_wkf <- workflow() |> add_recipe(MLR_recipe1) |> add_model(lasso_spec)
tree_wkf  <- workflow() |> add_recipe(MLR_recipe1) |> add_model(tree_spec)
bag_wkf   <- workflow() |> add_recipe(MLR_recipe1) |> add_model(bag_spec)
rf_wkf    <- workflow() |> add_recipe(MLR_recipe1) |> add_model(rf_spec)
```

## 7. Tuning

```{r}
# Setting up grids
lasso_grid <- grid_regular(penalty(), levels = 50)
tree_grid  <- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 5)
bag_grid   <- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 5)
rf_grid    <- grid_regular(mtry(range = c(1, 10)), min_n(), levels = 5)

# Tune Models
lasso_res <- tune_grid(lasso_wkf, resamples = bike_CV_folds, grid = lasso_grid)
tree_res  <- tune_grid(tree_wkf,  resamples = bike_CV_folds, grid = tree_grid)
bag_res   <- tune_grid(bag_wkf,   resamples = bike_CV_folds, grid = bag_grid)
rf_res    <- tune_grid(rf_wkf,    resamples = bike_CV_folds, grid = rf_grid)
```

## 8. Workflows

```{r}
# Select best parameters for each model 
best_lasso <- select_best(lasso_res, metric = "rmse")
final_lasso_wkf <- finalize_workflow(lasso_wkf, best_lasso)
final_lasso_fit <- last_fit(final_lasso_wkf, bike_split)

best_tree  <- select_best(tree_res, metric = "rmse")
final_tree_wkf  <- finalize_workflow(tree_wkf, best_tree)
final_tree_fit  <- last_fit(final_tree_wkf, bike_split)

best_bag   <- select_best(bag_res, metric = "rmse")
final_bag_wkf   <- finalize_workflow(bag_wkf, best_bag) 
final_bag_fit   <- last_fit(final_bag_wkf, bike_split)

best_rf    <- select_best(rf_res, metric = "rmse")
final_rf_wkf    <- finalize_workflow(rf_wkf, best_rf)
final_rf_fit    <- last_fit(final_rf_wkf, bike_split)
```

## 9. Best MLR model (Homework8)

```{r}
MLR_recipe3 <- MLR_recipe1 |>
  step_interact(~ starts_with("seasons_"):starts_with("holiday_")) |>
  step_interact(~ starts_with("seasons_"):temperature_mean_c) |>
  step_interact(~ temperature_mean_c:rainfall_sum_mm) |>
  step_poly(rainfall_sum_mm, snowfall_sum_cm, temperature_mean_c, humidity_mean_percent,
            wind_speed_mean_m_s, visibility_mean_10m, dew_point_temperature_mean_c,
            solar_radiation_mean_mj_m2, degree = 2)

MLR_wkf3 <- workflow() |> add_recipe(MLR_recipe3) |> add_model(linear_reg() |> set_engine("lm"))
final_mlr_fit <- last_fit(MLR_wkf3, bike_split)
```

## 10. Comparison

```{r}
# Collect metrics (RMSE and R squared) 
compare_models <- bind_rows(
  collect_metrics(final_lasso_fit) |> mutate(model = "LASSO"),
  collect_metrics(final_tree_fit) |> mutate(model = "Regression Tree"),
  collect_metrics(final_bag_fit) |> mutate(model = "Bagged Tree"),
  collect_metrics(final_rf_fit) |> mutate(model = "Random Forest"),
  collect_metrics(final_mlr_fit) |> mutate(model = "MLR (HW8 Best)")
) |>
  select(model, .metric, .estimate) |>
  pivot_wider(names_from = .metric, values_from = .estimate)

print(compare_models)
```

### Random Forest is the best model based on RMSE(lowest) and R Squared(highest).

## 11. Report

```{r}
# A. LASSO Coefficient Table 
lasso_coefs <- extract_fit_parsnip(final_lasso_fit) |>
  tidy() |>
  filter(estimate != 0)
print(lasso_coefs)

# B. Regression Tree Plot 
tree_engine <- extract_fit_engine(final_tree_fit)
rpart.plot(tree_engine, roundint = FALSE)

# C. Bagged Tree
bag_engine <- extract_fit_engine(final_bag_fit)

bag_importance <- bag_engine$imp |>
  select(term, value) |>
  rename(Variable = term, Importance = value) |>
  arrange(desc(Importance))

# Plot using ggplot
bag_plot <- bag_importance |>
  ggplot(aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_col(fill = "black", alpha = 0.7) +
  coord_flip() +
  labs(
    title = "Bagged Tree Variable Importance",
    x = "Predictor",
    y = "Importance"
  ) +
  theme_minimal()

print(bag_plot)

# D. Random Forest
rf_engine <- extract_fit_engine(final_rf_fit)

rf_importance <- tibble(
  Variable = names(rf_engine$variable.importance),
  Importance = rf_engine$variable.importance
) |>
  arrange(desc(Importance))

rf_plot <- rf_importance |>
  ggplot(aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_col(fill = "black", alpha = 0.7) +
  coord_flip() +
  labs(
    title = "Random Forest Variable Importance",
    x = "Predictor",
    y = "Importance (Impurity)"
  ) +
  theme_minimal()

print(rf_plot)
```

## 12. Fit the best model(Random Forest) on the full dataset

```{r}
final_model_full <- fit(final_rf_wkf, data = bike_data_daily)

print(final_model_full)
```
